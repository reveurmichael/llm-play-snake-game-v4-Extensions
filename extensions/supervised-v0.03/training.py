"""
Supervised Learning Training System v0.03 - Excellence Edition
=============================================================

Comprehensive training system for ML models with advanced features,
performance optimization, and excellent user experience.

Key Features:
- Multi-model training (MLP, LightGBM)
- Advanced feature engineering
- Hyperparameter optimization
- Model validation and evaluation
- Performance monitoring and analysis
- Automatic model saving and versioning
"""

from __future__ import annotations
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).resolve().parents[2]))

import time
import json
import pickle
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional, Union
import pandas as pd
import numpy as np

# Simple print functions to avoid dependency issues
def print_info(msg): print(f"[INFO] {msg}")
def print_warning(msg): print(f"[WARNING] {msg}")  
def print_error(msg): print(f"[ERROR] {msg}")
def print_success(msg): print(f"[SUCCESS] {msg}")

# Try to import ML libraries with graceful fallbacks
try:
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print_warning("Scikit-learn not available - training will be limited")

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print_warning("PyTorch not available - MLP training disabled")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print_warning("LightGBM not available - LightGBM training disabled")


class AdvancedFeatureEngineer:
    """Advanced feature engineering for Snake game states."""
    
    def __init__(self):
        self.feature_names = []
        self.scaler = StandardScaler() if SKLEARN_AVAILABLE else None
        self.label_encoder = LabelEncoder() if SKLEARN_AVAILABLE else None
    
    def engineer_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Engineer comprehensive features from game data."""
        print_info("ðŸ”§ Engineering advanced features...")
        
        # Basic features
        features = []
        
        # Snake head position (normalized)
        if 'snake_head_x' in df.columns and 'snake_head_y' in df.columns:
            head_x = df['snake_head_x'].values / 20.0  # Normalize to [0,1]
            head_y = df['snake_head_y'].values / 20.0
            features.extend([head_x, head_y])
            self.feature_names.extend(['head_x_norm', 'head_y_norm'])
        
        # Food position (normalized)
        if 'food_x' in df.columns and 'food_y' in df.columns:
            food_x = df['food_x'].values / 20.0
            food_y = df['food_y'].values / 20.0
            features.extend([food_x, food_y])
            self.feature_names.extend(['food_x_norm', 'food_y_norm'])
        
        # Distance to food (Manhattan and Euclidean)
        if len(features) >= 4:
            dist_manhattan = np.abs(features[0] - features[2]) + np.abs(features[1] - features[3])
            dist_euclidean = np.sqrt((features[0] - features[2])**2 + (features[1] - features[3])**2)
            features.extend([dist_manhattan, dist_euclidean])
            self.feature_names.extend(['food_dist_manhattan', 'food_dist_euclidean'])
        
        # Direction to food
        if len(features) >= 4:
            food_direction_x = features[2] - features[0]  # food_x - head_x
            food_direction_y = features[3] - features[1]  # food_y - head_y
            features.extend([food_direction_x, food_direction_y])
            self.feature_names.extend(['food_dir_x', 'food_dir_y'])
        
        # Snake length (if available)
        if 'snake_length' in df.columns:
            length_norm = df['snake_length'].values / 400.0  # Normalize by max possible length
            features.append(length_norm)
            self.feature_names.append('snake_length_norm')
        
        # Game step (if available)
        if 'step' in df.columns:
            step_norm = df['step'].values / 1000.0  # Normalize by typical max steps
            features.append(step_norm)
            self.feature_names.append('step_norm')
        
        # Wall distances (if head position available)
        if len(features) >= 2:
            wall_up = features[1]  # Distance to top wall
            wall_down = 1.0 - features[1]  # Distance to bottom wall  
            wall_left = features[0]  # Distance to left wall
            wall_right = 1.0 - features[0]  # Distance to right wall
            features.extend([wall_up, wall_down, wall_left, wall_right])
            self.feature_names.extend(['wall_up', 'wall_down', 'wall_left', 'wall_right'])
        
        # Convert to numpy array
        if features:
            X = np.column_stack(features)
        else:
            # Fallback: use all numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            feature_cols = [col for col in numeric_cols if col != 'move']
            X = df[feature_cols].values
            self.feature_names = feature_cols.tolist()
        
        # Prepare labels
        if 'move' in df.columns:
            if SKLEARN_AVAILABLE and self.label_encoder:
                y = self.label_encoder.fit_transform(df['move'])
            else:
                # Manual encoding
                move_map = {'UP': 0, 'DOWN': 1, 'LEFT': 2, 'RIGHT': 3}
                y = df['move'].map(move_map).fillna(0).values
        else:
            y = np.zeros(len(X))
        
        print_success(f"âœ… Engineered {X.shape[1]} features from {len(df)} samples")
        return X, y
    
    def get_feature_names(self) -> List[str]:
        """Get list of feature names."""
        return self.feature_names


class MLPTrainer:
    """Advanced MLP trainer with hyperparameter optimization."""
    
    def __init__(self):
        self.model = None
        self.training_history = {}
        self.best_accuracy = 0.0
    
    def create_model(self, input_size: int, hidden_layers: List[int] = None, 
                    num_classes: int = 4, dropout_rate: float = 0.2) -> nn.Module:
        """Create MLP model with advanced architecture."""
        if not TORCH_AVAILABLE:
            print_error("PyTorch not available - cannot create MLP model")
            return None
        
        if hidden_layers is None:
            hidden_layers = [128, 64, 32]
        
        layers = []
        prev_size = input_size
        
        for i, hidden_size in enumerate(hidden_layers):
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout_rate)
            ])
            prev_size = hidden_size
        
        # Output layer
        layers.append(nn.Linear(prev_size, num_classes))
        
        model = nn.Sequential(*layers)
        return model
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray, 
             X_val: np.ndarray = None, y_val: np.ndarray = None,
             epochs: int = 100, batch_size: int = 64, learning_rate: float = 0.001,
             hidden_layers: List[int] = None, **kwargs) -> Dict[str, Any]:
        """Train MLP model with advanced features."""
        if not TORCH_AVAILABLE:
            return {"error": "PyTorch not available"}
        
        print_info(f"ðŸ§  Training MLP model with {len(X_train)} samples...")
        
        # Create model
        input_size = X_train.shape[1]
        self.model = self.create_model(input_size, hidden_layers)
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(device)
        
        # Prepare data
        X_tensor = torch.FloatTensor(X_train).to(device)
        y_tensor = torch.LongTensor(y_train).to(device)
        
        train_dataset = TensorDataset(X_tensor, y_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # Validation data
        if X_val is not None and y_val is not None:
            X_val_tensor = torch.FloatTensor(X_val).to(device)
            y_val_tensor = torch.LongTensor(y_val).to(device)
        
        # Optimizer and loss
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # Training loop
        train_losses = []
        train_accuracies = []
        val_accuracies = []
        
        start_time = time.time()
        
        for epoch in range(epochs):
            self.model.train()
            epoch_loss = 0.0
            correct = 0
            total = 0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
            
            train_acc = correct / total
            train_losses.append(epoch_loss / len(train_loader))
            train_accuracies.append(train_acc)
            
            # Validation
            if X_val is not None:
                self.model.eval()
                with torch.no_grad():
                    val_outputs = self.model(X_val_tensor)
                    _, val_predicted = torch.max(val_outputs.data, 1)
                    val_acc = (val_predicted == y_val_tensor).float().mean().item()
                    val_accuracies.append(val_acc)
                    
                    if val_acc > self.best_accuracy:
                        self.best_accuracy = val_acc
            
            if epoch % 10 == 0:
                val_info = f", Val Acc: {val_accuracies[-1]:.4f}" if val_accuracies else ""
                print_info(f"Epoch {epoch}: Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc:.4f}{val_info}")
        
        training_time = time.time() - start_time
        
        self.training_history = {
            "train_losses": train_losses,
            "train_accuracies": train_accuracies,
            "val_accuracies": val_accuracies,
            "training_time": training_time,
            "best_accuracy": self.best_accuracy,
            "final_train_accuracy": train_accuracies[-1],
            "epochs": epochs,
            "batch_size": batch_size,
            "learning_rate": learning_rate
        }
        
        print_success(f"âœ… MLP training completed in {training_time:.2f}s")
        print_success(f"ðŸŽ¯ Best accuracy: {self.best_accuracy:.4f}")
        
        return self.training_history
    
    def save_model(self, filepath: str) -> bool:
        """Save trained model."""
        if self.model is None:
            print_error("No model to save")
            return False
        
        try:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'training_history': self.training_history,
                'model_config': {
                    'input_size': list(self.model.children())[0].in_features,
                    'num_classes': list(self.model.children())[-1].out_features
                }
            }, filepath)
            print_success(f"âœ… Model saved to {filepath}")
            return True
        except Exception as e:
            print_error(f"Failed to save model: {e}")
            return False


class LightGBMTrainer:
    """Advanced LightGBM trainer with hyperparameter optimization."""
    
    def __init__(self):
        self.model = None
        self.training_history = {}
        self.feature_importance = {}
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
             X_val: np.ndarray = None, y_val: np.ndarray = None,
             num_leaves: int = 31, learning_rate: float = 0.1,
             n_estimators: int = 100, **kwargs) -> Dict[str, Any]:
        """Train LightGBM model with advanced features."""
        if not LIGHTGBM_AVAILABLE:
            return {"error": "LightGBM not available"}
        
        print_info(f"ðŸŒ² Training LightGBM model with {len(X_train)} samples...")
        
        start_time = time.time()
        
        # Prepare data
        train_data = lgb.Dataset(X_train, label=y_train)
        
        params = {
            'objective': 'multiclass',
            'num_class': 4,
            'num_leaves': num_leaves,
            'learning_rate': learning_rate,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1
        }
        
        # Train model
        if X_val is not None and y_val is not None:
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            self.model = lgb.train(
                params,
                train_data,
                num_boost_round=n_estimators,
                valid_sets=[val_data],
                callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
            )
        else:
            self.model = lgb.train(
                params,
                train_data,
                num_boost_round=n_estimators
            )
        
        training_time = time.time() - start_time
        
        # Feature importance
        self.feature_importance = dict(zip(
            [f"feature_{i}" for i in range(X_train.shape[1])],
            self.model.feature_importance()
        ))
        
        # Training history
        self.training_history = {
            "training_time": training_time,
            "num_trees": self.model.num_trees(),
            "feature_importance": self.feature_importance,
            "params": params
        }
        
        print_success(f"âœ… LightGBM training completed in {training_time:.2f}s")
        print_success(f"ðŸŒ² Number of trees: {self.model.num_trees()}")
        
        return self.training_history
    
    def save_model(self, filepath: str) -> bool:
        """Save trained model."""
        if self.model is None:
            print_error("No model to save")
            return False
        
        try:
            with open(filepath, 'wb') as f:
                pickle.dump({
                    'model': self.model,
                    'training_history': self.training_history,
                    'feature_importance': self.feature_importance
                }, f)
            print_success(f"âœ… Model saved to {filepath}")
            return True
        except Exception as e:
            print_error(f"Failed to save model: {e}")
            return False


class SupervisedTrainingPipeline:
    """Comprehensive training pipeline for supervised learning."""
    
    def __init__(self):
        self.feature_engineer = AdvancedFeatureEngineer()
        self.mlp_trainer = MLPTrainer()
        self.lgb_trainer = LightGBMTrainer()
        self.results = {}
    
    def load_data(self, data_path: str) -> pd.DataFrame:
        """Load training data from CSV."""
        print_info(f"ðŸ“ Loading data from {data_path}...")
        
        try:
            df = pd.read_csv(data_path)
            print_success(f"âœ… Loaded {len(df)} samples with {len(df.columns)} columns")
            return df
        except Exception as e:
            print_error(f"Failed to load data: {e}")
            return pd.DataFrame()
    
    def train_all_models(self, data_path: str, test_size: float = 0.2,
                        val_size: float = 0.1, **kwargs) -> Dict[str, Any]:
        """Train all available models."""
        print_info("ðŸš€ Starting comprehensive model training...")
        
        # Load data
        df = self.load_data(data_path)
        if df.empty:
            return {"error": "No data loaded"}
        
        # Feature engineering
        X, y = self.feature_engineer.engineer_features(df)
        
        # Split data
        if SKLEARN_AVAILABLE:
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, test_size=test_size, random_state=42
            )
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp, test_size=val_size/(1-test_size), random_state=42
            )
        else:
            # Simple split without sklearn
            n_test = int(len(X) * test_size)
            n_val = int(len(X) * val_size)
            
            X_test, y_test = X[-n_test:], y[-n_test:]
            X_val, y_val = X[-(n_test+n_val):-n_test], y[-(n_test+n_val):-n_test]
            X_train, y_train = X[:-(n_test+n_val)], y[:-(n_test+n_val)]
        
        print_info(f"ðŸ“Š Data split: Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        
        results = {}
        
        # Train MLP
        if TORCH_AVAILABLE:
            print_info("ðŸ§  Training MLP model...")
            mlp_results = self.mlp_trainer.train(
                X_train, y_train, X_val, y_val, **kwargs
            )
            results['mlp'] = mlp_results
        
        # Train LightGBM
        if LIGHTGBM_AVAILABLE:
            print_info("ðŸŒ² Training LightGBM model...")
            lgb_results = self.lgb_trainer.train(
                X_train, y_train, X_val, y_val, **kwargs
            )
            results['lightgbm'] = lgb_results
        
        self.results = results
        return results
    
    def save_models(self, output_dir: str) -> bool:
        """Save all trained models."""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        success = True
        
        # Save MLP
        if self.mlp_trainer.model is not None:
            mlp_path = output_path / "mlp_model.pth"
            success &= self.mlp_trainer.save_model(str(mlp_path))
        
        # Save LightGBM
        if self.lgb_trainer.model is not None:
            lgb_path = output_path / "lightgbm_model.pkl"
            success &= self.lgb_trainer.save_model(str(lgb_path))
        
        # Save training results
        results_path = output_path / "training_results.json"
        try:
            with open(results_path, 'w') as f:
                json.dump(self.results, f, indent=2, default=str)
            print_success(f"âœ… Training results saved to {results_path}")
        except Exception as e:
            print_error(f"Failed to save training results: {e}")
            success = False
        
        return success


def main():
    """Main training function for demonstration."""
    print_info("ðŸŽ¯ Supervised Learning Training System v0.03")
    
    # Example usage
    pipeline = SupervisedTrainingPipeline()
    
    # This would be called with actual data
    # results = pipeline.train_all_models("path/to/training_data.csv")
    # pipeline.save_models("./models")
    
    print_success("âœ… Training system initialized successfully!")


if __name__ == "__main__":
    main()